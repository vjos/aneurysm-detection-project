{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl\n",
    "\n",
    "import augmentation\n",
    "import loss\n",
    "import dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from intra import IntrA\n",
    "from models.pointnet import PointNetCls\n",
    "from models.pointnetcls import PointNet\n",
    "from models.pointnet2 import PointNet2\n",
    "from models.pointconv import PointConvDensityClsSsg\n",
    "from models.pointmlp import pointMLP\n",
    "from models.dgcnn import DGCNN\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import train_model, train_kfold_intra, eval_model_classification\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "dotenv.load_dotenv()  # load the MLflow http authentication parameters\n",
    "mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\"))\n",
    "\n",
    "dev = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jventers/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/Users/jventers/Documents/masters-project/project/models/pointnet2_utils.py:113: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:39.)\n",
      "  group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m train_dl \u001b[39m=\u001b[39m DataLoader(trn, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m test_dl \u001b[39m=\u001b[39m DataLoader(tst, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m train_model(\n\u001b[1;32m     20\u001b[0m     model,\n\u001b[1;32m     21\u001b[0m     train_dl,\n\u001b[1;32m     22\u001b[0m     test_dl,\n\u001b[1;32m     23\u001b[0m     F\u001b[39m.\u001b[39;49mnll_loss,\n\u001b[1;32m     24\u001b[0m     augmentation\u001b[39m.\u001b[39;49maug_dgcnn,\n\u001b[1;32m     25\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     checkpoint_epoch\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPointNet2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     trans_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     29\u001b[0m     opt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     sched\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:123\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, aug, epochs, checkpoint_epoch, model_name, opt, sched, lr, momentum, snapshot_path, trans_loss)\u001b[0m\n\u001b[1;32m    121\u001b[0m mlflow\u001b[39m.\u001b[39mlog_params({x: \u001b[39mstr\u001b[39m(y) \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mlocals\u001b[39m()\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 123\u001b[0m     train_step(\n\u001b[1;32m    124\u001b[0m         model,\n\u001b[1;32m    125\u001b[0m         scheduler,\n\u001b[1;32m    126\u001b[0m         optimizer,\n\u001b[1;32m    127\u001b[0m         train,\n\u001b[1;32m    128\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    129\u001b[0m         aug\u001b[39m=\u001b[39;49maug,\n\u001b[1;32m    130\u001b[0m         trans_loss\u001b[39m=\u001b[39;49mtrans_loss,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     train_metrics \u001b[39m=\u001b[39m eval_model_classification(model, train, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m     test_metrics \u001b[39m=\u001b[39m eval_model_classification(model, test, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:56\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, scheduler, optimizer, data, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m     54\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, label, trans_feat)\n\u001b[1;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     pred \u001b[39m=\u001b[39m model(batch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, label)\n\u001b[1;32m     59\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2.py:51\u001b[0m, in \u001b[0;36mPointNet2.forward\u001b[0;34m(self, xyz)\u001b[0m\n\u001b[1;32m     49\u001b[0m     norm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m l1_xyz, l1_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa1(xyz, norm)\n\u001b[0;32m---> 51\u001b[0m l2_xyz, l2_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa2(l1_xyz, l1_points)\n\u001b[1;32m     52\u001b[0m l3_xyz, l3_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa3(l2_xyz, l2_points)\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m l3_points\u001b[39m.\u001b[39mview(B, \u001b[39m1024\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:204\u001b[0m, in \u001b[0;36mPointNetSetAbstraction.forward\u001b[0;34m(self, xyz, points)\u001b[0m\n\u001b[1;32m    202\u001b[0m     new_xyz, new_points \u001b[39m=\u001b[39m sample_and_group_all(xyz, points)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     new_xyz, new_points \u001b[39m=\u001b[39m sample_and_group(\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnpoint, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mradius, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnsample, xyz, points\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39m# new_xyz: sampled points position data, [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# new_points: sampled points data, [B, npoint, nsample, C+D]\u001b[39;00m\n\u001b[1;32m    209\u001b[0m new_points \u001b[39m=\u001b[39m new_points\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# [B, C+D, nsample,npoint]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:134\u001b[0m, in \u001b[0;36msample_and_group\u001b[0;34m(npoint, radius, nsample, xyz, points, returnfps)\u001b[0m\n\u001b[1;32m    132\u001b[0m B, N, C \u001b[39m=\u001b[39m xyz\u001b[39m.\u001b[39mshape\n\u001b[1;32m    133\u001b[0m S \u001b[39m=\u001b[39m npoint\n\u001b[0;32m--> 134\u001b[0m fps_idx \u001b[39m=\u001b[39m farthest_point_sample(xyz, npoint)  \u001b[39m# [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    135\u001b[0m new_xyz \u001b[39m=\u001b[39m index_points(xyz, fps_idx)\n\u001b[1;32m    136\u001b[0m idx \u001b[39m=\u001b[39m query_ball_point(radius, nsample, xyz, new_xyz)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:90\u001b[0m, in \u001b[0;36mfarthest_point_sample\u001b[0;34m(xyz, npoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m     dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum((xyz \u001b[39m-\u001b[39m centroid) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     89\u001b[0m     mask \u001b[39m=\u001b[39m dist \u001b[39m<\u001b[39m distance\n\u001b[0;32m---> 90\u001b[0m     distance[mask] \u001b[39m=\u001b[39m dist[mask]\n\u001b[1;32m     91\u001b[0m     farthest \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(distance, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m centroids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Example training setup for single epoch PointNet.\"\"\"\n",
    "if False:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"./data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = PointNet2(2, normal_channel=True)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=8, num_workers=0, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=0, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        F.nll_loss,\n",
    "        augmentation.aug_dgcnn,\n",
    "        epochs=1,\n",
    "        checkpoint_epoch=50,\n",
    "        model_name=\"PointNet2\",\n",
    "        trans_loss=False,\n",
    "        opt=\"adam\",\n",
    "        sched=\"step\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/generated/vessel/ad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"DGCNN full training setup.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39m# train pointnet for a single epoch to test utils on a slow laptop\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[39m=\u001b[39m IntrA(\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         npoints\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m         exclude_seg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m         norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     trn, tst \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [\u001b[39m0.8\u001b[39m, \u001b[39m0.2\u001b[39m], torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m))\n\u001b[1;32m     14\u001b[0m     model \u001b[39m=\u001b[39m DGCNN(output_channels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:46\u001b[0m, in \u001b[0;36mIntrA.__init__\u001b[0;34m(self, root, dataset, npoints, exclude_seg, norm, fold, kfold_splits, test)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mvessel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maneurysm\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 46\u001b[0m         paths \u001b[39m=\u001b[39m get_paths(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, \u001b[39m\"\u001b[39;49m\u001b[39mgenerated\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mcls\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mad\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     47\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m paths\n\u001b[1;32m     48\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [i] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(paths)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:102\u001b[0m, in \u001b[0;36mget_paths\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_paths\u001b[39m(\u001b[39mdir\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns list of full paths of all pointcloud files in a given directory.\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    101\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, f)\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mdir\u001b[39;49m)\n\u001b[1;32m    103\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(f)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m.ad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.obj\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m     ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/generated/vessel/ad'"
     ]
    }
   ],
   "source": [
    "\"\"\"DGCNN full training setup.\"\"\"\n",
    "if False:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = DGCNN(output_channels=2, norm=True)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=32, num_workers=8, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=16, num_workers=4, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        augmentation.aug_dgcnn,\n",
    "        model_name=\"DGCNN\",\n",
    "        epochs=200,\n",
    "        checkpoint_epoch=50,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PointMLP full training setup.\"\"\"\n",
    "if True:\n",
    "    dataset = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = pointMLP(num_classes=2)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=16, num_workers=4, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=2, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        None,\n",
    "        model_name=\"PointMLP\",\n",
    "        epochs=200,\n",
    "        checkpoint_epoch=50,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.1,\n",
    "        min_lr=0.005\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_class': <class 'models.pointnetcls.PointNet'>, 'model_kwargs': {'k': 2}, 'epochs': 1, 'batch_size': 8, 'num_workers': 2, 'norm': True, 'checkpoint_epoch': None, 'model_name': 'PointNet', 'intra_root': './data', 'npoints': 1024, 'exclude_seg': True, 'snapshot_path': './snapshots', 'splits': './file_splits/cls/', 'loss_fn': <function nll_loss at 0x144a383a0>, 'aug': None, 'trans_loss': False}\n",
      "\n",
      "F1:\n",
      "Epoch: 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jventers/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_kfold_intra(\n\u001b[1;32m      2\u001b[0m     PointNet,\n\u001b[1;32m      3\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mk\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m2\u001b[39;49m},\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPointNet\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     intra_root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     npoints\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     splits\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./file_splits/cls/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     exclude_seg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:206\u001b[0m, in \u001b[0;36mtrain_kfold_intra\u001b[0;34m(model_class, model_kwargs, epochs, batch_size, num_workers, norm, checkpoint_epoch, model_name, intra_root, npoints, exclude_seg, snapshot_path, splits, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     train_step(\n\u001b[1;32m    207\u001b[0m         model,\n\u001b[1;32m    208\u001b[0m         scheduler,\n\u001b[1;32m    209\u001b[0m         optimizer,\n\u001b[1;32m    210\u001b[0m         train_dl,\n\u001b[1;32m    211\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    212\u001b[0m         aug\u001b[39m=\u001b[39;49maug,\n\u001b[1;32m    213\u001b[0m         trans_loss\u001b[39m=\u001b[39;49mtrans_loss,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     train_metrics \u001b[39m=\u001b[39m eval_model_classification(\n\u001b[1;32m    217\u001b[0m         model, train_dl, norm\u001b[39m=\u001b[39mnorm, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_train_\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    220\u001b[0m     test_metrics \u001b[39m=\u001b[39m eval_model_classification(\n\u001b[1;32m    221\u001b[0m         model, test_dl, norm\u001b[39m=\u001b[39mnorm, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_test_\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:40\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, scheduler, optimizer, data, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train the model once on the given dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m batch, label \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_kfold_intra(\n",
    "    DGCNN,\n",
    "    {\"output_channels\":2, \"norm\":True},\n",
    "    loss.smooth_cross_entropy_loss(),\n",
    "    augmentation.aug_dgcnn,\n",
    "    epochs=200,\n",
    "    model_name=\"DGCNN\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    intra_root=\"/data\",\n",
    "    npoints=1024,\n",
    "    norm=True,\n",
    "    splits=\"./file_splits/cls/\",\n",
    "    exclude_seg=True,\n",
    "    opt=\"adam\",\n",
    "    sched=\"step\",\n",
    "    lr=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
