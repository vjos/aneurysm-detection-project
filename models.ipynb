{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl\n",
    "\n",
    "import augmentation\n",
    "import loss\n",
    "import dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from intra import IntrA\n",
    "from models.pointnet import PointNetCls\n",
    "from models.pointnetcls import PointNet\n",
    "from models.pointnet2 import PointNet2\n",
    "from models.pointconv import PointConvDensityClsSsg\n",
    "from models.pointmlp import pointMLP\n",
    "from models.dgcnn import DGCNN\n",
    "from models.curvenet import CurveNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import train_model, train_kfold_intra, eval_model_classification\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "dotenv.load_dotenv()  # load the MLflow http authentication parameters\n",
    "mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\"))\n",
    "\n",
    "dev = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jventers/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/Users/jventers/Documents/masters-project/project/models/pointnet2_utils.py:113: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Sort.mm:39.)\n",
      "  group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m train_dl \u001b[39m=\u001b[39m DataLoader(trn, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m test_dl \u001b[39m=\u001b[39m DataLoader(tst, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m train_model(\n\u001b[1;32m     20\u001b[0m     model,\n\u001b[1;32m     21\u001b[0m     train_dl,\n\u001b[1;32m     22\u001b[0m     test_dl,\n\u001b[1;32m     23\u001b[0m     F\u001b[39m.\u001b[39;49mnll_loss,\n\u001b[1;32m     24\u001b[0m     augmentation\u001b[39m.\u001b[39;49maug_dgcnn,\n\u001b[1;32m     25\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     checkpoint_epoch\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPointNet2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     trans_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     29\u001b[0m     opt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     sched\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:123\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, aug, epochs, checkpoint_epoch, model_name, opt, sched, lr, momentum, snapshot_path, trans_loss)\u001b[0m\n\u001b[1;32m    121\u001b[0m mlflow\u001b[39m.\u001b[39mlog_params({x: \u001b[39mstr\u001b[39m(y) \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mlocals\u001b[39m()\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 123\u001b[0m     train_step(\n\u001b[1;32m    124\u001b[0m         model,\n\u001b[1;32m    125\u001b[0m         scheduler,\n\u001b[1;32m    126\u001b[0m         optimizer,\n\u001b[1;32m    127\u001b[0m         train,\n\u001b[1;32m    128\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    129\u001b[0m         aug\u001b[39m=\u001b[39;49maug,\n\u001b[1;32m    130\u001b[0m         trans_loss\u001b[39m=\u001b[39;49mtrans_loss,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     train_metrics \u001b[39m=\u001b[39m eval_model_classification(model, train, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m     test_metrics \u001b[39m=\u001b[39m eval_model_classification(model, test, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:56\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, scheduler, optimizer, data, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m     54\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, label, trans_feat)\n\u001b[1;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     pred \u001b[39m=\u001b[39m model(batch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, label)\n\u001b[1;32m     59\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2.py:51\u001b[0m, in \u001b[0;36mPointNet2.forward\u001b[0;34m(self, xyz)\u001b[0m\n\u001b[1;32m     49\u001b[0m     norm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m l1_xyz, l1_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa1(xyz, norm)\n\u001b[0;32m---> 51\u001b[0m l2_xyz, l2_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa2(l1_xyz, l1_points)\n\u001b[1;32m     52\u001b[0m l3_xyz, l3_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa3(l2_xyz, l2_points)\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m l3_points\u001b[39m.\u001b[39mview(B, \u001b[39m1024\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:204\u001b[0m, in \u001b[0;36mPointNetSetAbstraction.forward\u001b[0;34m(self, xyz, points)\u001b[0m\n\u001b[1;32m    202\u001b[0m     new_xyz, new_points \u001b[39m=\u001b[39m sample_and_group_all(xyz, points)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     new_xyz, new_points \u001b[39m=\u001b[39m sample_and_group(\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnpoint, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mradius, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnsample, xyz, points\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39m# new_xyz: sampled points position data, [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# new_points: sampled points data, [B, npoint, nsample, C+D]\u001b[39;00m\n\u001b[1;32m    209\u001b[0m new_points \u001b[39m=\u001b[39m new_points\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# [B, C+D, nsample,npoint]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:134\u001b[0m, in \u001b[0;36msample_and_group\u001b[0;34m(npoint, radius, nsample, xyz, points, returnfps)\u001b[0m\n\u001b[1;32m    132\u001b[0m B, N, C \u001b[39m=\u001b[39m xyz\u001b[39m.\u001b[39mshape\n\u001b[1;32m    133\u001b[0m S \u001b[39m=\u001b[39m npoint\n\u001b[0;32m--> 134\u001b[0m fps_idx \u001b[39m=\u001b[39m farthest_point_sample(xyz, npoint)  \u001b[39m# [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    135\u001b[0m new_xyz \u001b[39m=\u001b[39m index_points(xyz, fps_idx)\n\u001b[1;32m    136\u001b[0m idx \u001b[39m=\u001b[39m query_ball_point(radius, nsample, xyz, new_xyz)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/models/pointnet2_utils.py:90\u001b[0m, in \u001b[0;36mfarthest_point_sample\u001b[0;34m(xyz, npoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m     dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum((xyz \u001b[39m-\u001b[39m centroid) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     89\u001b[0m     mask \u001b[39m=\u001b[39m dist \u001b[39m<\u001b[39m distance\n\u001b[0;32m---> 90\u001b[0m     distance[mask] \u001b[39m=\u001b[39m dist[mask]\n\u001b[1;32m     91\u001b[0m     farthest \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(distance, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m centroids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Example training setup for single epoch PointNet.\"\"\"\n",
    "if False:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"./data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = PointNet2(2, normal_channel=True)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=8, num_workers=0, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=0, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        F.nll_loss,\n",
    "        augmentation.aug_dgcnn,\n",
    "        epochs=1,\n",
    "        checkpoint_epoch=50,\n",
    "        model_name=\"PointNet2\",\n",
    "        trans_loss=False,\n",
    "        opt=\"adam\",\n",
    "        sched=\"step\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/generated/vessel/ad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"DGCNN full training setup.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39m# train pointnet for a single epoch to test utils on a slow laptop\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[39m=\u001b[39m IntrA(\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         npoints\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m         exclude_seg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m         norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     trn, tst \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [\u001b[39m0.8\u001b[39m, \u001b[39m0.2\u001b[39m], torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m))\n\u001b[1;32m     14\u001b[0m     model \u001b[39m=\u001b[39m DGCNN(output_channels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:46\u001b[0m, in \u001b[0;36mIntrA.__init__\u001b[0;34m(self, root, dataset, npoints, exclude_seg, norm, fold, kfold_splits, test)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mvessel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maneurysm\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 46\u001b[0m         paths \u001b[39m=\u001b[39m get_paths(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, \u001b[39m\"\u001b[39;49m\u001b[39mgenerated\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mcls\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mad\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     47\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m paths\n\u001b[1;32m     48\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [i] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(paths)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:102\u001b[0m, in \u001b[0;36mget_paths\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_paths\u001b[39m(\u001b[39mdir\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns list of full paths of all pointcloud files in a given directory.\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    101\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, f)\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mdir\u001b[39;49m)\n\u001b[1;32m    103\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(f)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m.ad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.obj\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m     ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/generated/vessel/ad'"
     ]
    }
   ],
   "source": [
    "\"\"\"DGCNN full training setup.\"\"\"\n",
    "if False:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = DGCNN(output_channels=2, norm=True)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=32, num_workers=8, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=16, num_workers=4, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        augmentation.aug_dgcnn,\n",
    "        model_name=\"DGCNN\",\n",
    "        epochs=200,\n",
    "        checkpoint_epoch=50,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PointMLP full training setup.\"\"\"\n",
    "if False:\n",
    "    trn = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\",\n",
    "        fold=1,\n",
    "        kfold_splits=\"./file_splits/cls\",\n",
    "        test=False,\n",
    "    )\n",
    "\n",
    "    tst = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\",\n",
    "        fold=1,\n",
    "        kfold_splits=\"./file_splits/cls\",\n",
    "        test=True,\n",
    "    )\n",
    " \n",
    "    train_dl = DataLoader(trn, batch_size=16, num_workers=4, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=2, drop_last=False)\n",
    "\n",
    "    model = pointMLP(num_classes=2)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        None,\n",
    "        model_name=\"PointMLP\",\n",
    "        epochs=200,\n",
    "        checkpoint_epoch=50,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.1,\n",
    "        min_lr=0.005\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CurveNet full training setup.\"\"\"\n",
    "if True:\n",
    "    trn = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\",\n",
    "        fold=1,\n",
    "        kfold_splits=\"./file_splits/cls\",\n",
    "        test=False,\n",
    "    )\n",
    "\n",
    "    tst = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\",\n",
    "        fold=1,\n",
    "        kfold_splits=\"./file_splits/cls\",\n",
    "        test=True,\n",
    "    )\n",
    " \n",
    "    train_dl = DataLoader(trn, batch_size=64, num_workers=8, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=2, drop_last=False)\n",
    "\n",
    "    model = CurveNet(num_classes=2)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        None,\n",
    "        model_name=\"CurveNet\",\n",
    "        epochs=300,\n",
    "        checkpoint_epoch=150,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.1,\n",
    "        min_lr=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=False,\n",
    "        dataset=\"classification\")\n",
    "\n",
    "test_dl = DataLoader(dataset, batch_size=4, num_workers=2, drop_last=False)\n",
    "\n",
    "model = pointMLP(num_classes=2)\n",
    "model = model.to(dev)\n",
    "model = torch.nn.DataParallel(model)\n",
    "state_dict = torch.load(\"200epochs.pt\")[\"state_dict\"]\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "eval_model_classification(model, test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
