{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl\n",
    "\n",
    "import augmentation\n",
    "import loss\n",
    "import dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from intra import IntrA\n",
    "from models.pointnet import PointNetCls\n",
    "from models.pointnetcls import PointNet\n",
    "from models.pointnet2 import PointNet2\n",
    "from models.pointconv import PointConvDensityClsSsg\n",
    "from models.dgcnn import DGCNN\n",
    "from utils import train_model, train_kfold_intra, eval_model_classification\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "dotenv.load_dotenv()  # load the MLflow http authentication parameters\n",
    "mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\"))\n",
    "\n",
    "dev = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example training setup for single epoch PointNet.\"\"\"\n",
    "if True:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"./data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = PointNet(k=2)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=8, num_workers=0, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=8, num_workers=0, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.trans_feat_loss(),\n",
    "        augmentation.aug_dgcnn,\n",
    "        epochs=1,\n",
    "        checkpoint_epoch=50,\n",
    "        model_name=\"PointNet\",\n",
    "        trans_loss=True,\n",
    "        opt=\"adam\",\n",
    "        sched=\"step\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/generated/vessel/ad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"DGCNN full training setup.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39m# train pointnet for a single epoch to test utils on a slow laptop\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[39m=\u001b[39m IntrA(\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         npoints\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m         exclude_seg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m         norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     trn, tst \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [\u001b[39m0.8\u001b[39m, \u001b[39m0.2\u001b[39m], torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m))\n\u001b[1;32m     14\u001b[0m     model \u001b[39m=\u001b[39m DGCNN(output_channels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:46\u001b[0m, in \u001b[0;36mIntrA.__init__\u001b[0;34m(self, root, dataset, npoints, exclude_seg, norm, fold, kfold_splits, test)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mvessel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maneurysm\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 46\u001b[0m         paths \u001b[39m=\u001b[39m get_paths(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, \u001b[39m\"\u001b[39;49m\u001b[39mgenerated\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mcls\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mad\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     47\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m paths\n\u001b[1;32m     48\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [i] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(paths)\n",
      "File \u001b[0;32m~/Documents/masters-project/project/intra.py:102\u001b[0m, in \u001b[0;36mget_paths\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_paths\u001b[39m(\u001b[39mdir\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns list of full paths of all pointcloud files in a given directory.\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    101\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, f)\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mdir\u001b[39;49m)\n\u001b[1;32m    103\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(f)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m.ad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.obj\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m     ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/generated/vessel/ad'"
     ]
    }
   ],
   "source": [
    "\"\"\"DGCNN full training setup.\"\"\"\n",
    "if True:\n",
    "    # train pointnet for a single epoch to test utils on a slow laptop\n",
    "    dataset = IntrA(\n",
    "        \"/data\",\n",
    "        npoints=1024,\n",
    "        exclude_seg=True,\n",
    "        norm=True,\n",
    "        dataset=\"classification\",\n",
    "    )\n",
    "\n",
    "    trn, tst = torch.utils.data.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    " \n",
    "    model = DGCNN(output_channels=2, norm=True)\n",
    "\n",
    "    train_dl = DataLoader(trn, batch_size=32, num_workers=8, drop_last=True, shuffle=True)\n",
    "    test_dl = DataLoader(tst, batch_size=16, num_workers=4, drop_last=False)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss.smooth_cross_entropy_loss(),\n",
    "        augmentation.aug_dgcnn,\n",
    "        model_name=\"DGCNN\",\n",
    "        epochs=200,\n",
    "        checkpoint_epoch=50,\n",
    "        trans_loss=False,\n",
    "        opt=\"sgd\",\n",
    "        sched=\"cosine\",\n",
    "        lr=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_class': <class 'models.pointnetcls.PointNet'>, 'model_kwargs': {'k': 2}, 'epochs': 1, 'batch_size': 8, 'num_workers': 2, 'norm': True, 'checkpoint_epoch': None, 'model_name': 'PointNet', 'intra_root': './data', 'npoints': 1024, 'exclude_seg': True, 'snapshot_path': './snapshots', 'splits': './file_splits/cls/', 'loss_fn': <function nll_loss at 0x144a383a0>, 'aug': None, 'trans_loss': False}\n",
      "\n",
      "F1:\n",
      "Epoch: 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jventers/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_kfold_intra(\n\u001b[1;32m      2\u001b[0m     PointNet,\n\u001b[1;32m      3\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mk\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m2\u001b[39;49m},\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPointNet\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     intra_root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     npoints\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     splits\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./file_splits/cls/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     exclude_seg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:206\u001b[0m, in \u001b[0;36mtrain_kfold_intra\u001b[0;34m(model_class, model_kwargs, epochs, batch_size, num_workers, norm, checkpoint_epoch, model_name, intra_root, npoints, exclude_seg, snapshot_path, splits, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     train_step(\n\u001b[1;32m    207\u001b[0m         model,\n\u001b[1;32m    208\u001b[0m         scheduler,\n\u001b[1;32m    209\u001b[0m         optimizer,\n\u001b[1;32m    210\u001b[0m         train_dl,\n\u001b[1;32m    211\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    212\u001b[0m         aug\u001b[39m=\u001b[39;49maug,\n\u001b[1;32m    213\u001b[0m         trans_loss\u001b[39m=\u001b[39;49mtrans_loss,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     train_metrics \u001b[39m=\u001b[39m eval_model_classification(\n\u001b[1;32m    217\u001b[0m         model, train_dl, norm\u001b[39m=\u001b[39mnorm, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_train_\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    220\u001b[0m     test_metrics \u001b[39m=\u001b[39m eval_model_classification(\n\u001b[1;32m    221\u001b[0m         model, test_dl, norm\u001b[39m=\u001b[39mnorm, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_test_\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/masters-project/project/utils.py:40\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, scheduler, optimizer, data, loss_fn, aug, trans_loss)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train the model once on the given dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m batch, label \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/masters-env/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_kfold_intra(\n",
    "    DGCNN,\n",
    "    {\"output_channels\":2, \"norm\":True},\n",
    "    epochs=200,\n",
    "    model_name=\"DGCNN\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    intra_root=\"/data\",\n",
    "    npoints=1024,\n",
    "    norm=True,\n",
    "    splits=\"./file_splits/cls/\",\n",
    "    exclude_seg=True,\n",
    "    opt=\"adam\"\n",
    "    sched=\"step\",\n",
    "    lr=0.001\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
